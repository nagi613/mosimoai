import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
import random

st.title("ğŸ”®ã‚‚ã—ã‚‚AIï¼ˆIFã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰")
st.write("ã‚ãªãŸã®ã€ã‚‚ã—ã‚‚ã€œã—ãŸã‚‰ï¼Ÿã€ã®æœªæ¥ã‚’ã€AIãŒæ—¥æœ¬èªã§äºˆæ¸¬ã—ã¾ã™ã€‚")

# å…¥åŠ›æ¬„
user_input = st.text_input("ã‚‚ã—ã‚‚â—‹â—‹ã—ãŸã‚‰ï¼Ÿï¼ˆä¾‹ï¼šã‚‚ã—ä»Šæ—¥å‹‰å¼·ã‚’ã‚µãƒœã£ãŸã‚‰ï¼Ÿï¼‰")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆæœ€åˆã ã‘ï¼‰
@st.cache_resource
def load_model():
    model_name = "rinna/japanese-gpt2-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

generator = load_model()

# ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚
if st.button("äºˆæ¸¬ã™ã‚‹ï¼"):
    if user_input.strip() == "":
        st.warning("ã€ã‚‚ã—ã‚‚ã€œã€ã®æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("AIãŒæœªæ¥ã‚’äºˆæ¸¬ä¸­...ğŸ”®"):
            result = generator(
                user_input,
                max_length=80,
                do_sample=True,
                temperature=0.9,
                top_p=0.95,
                num_return_sequences=1
            )[0]['generated_text']

        st.subheader("âœ¨ AIã®æœªæ¥äºˆæ¸¬ âœ¨")
        st.write(result)
        st.caption(random.choice([
            "â€»ã“ã®æœªæ¥ã¯AIã®æƒ³åƒã§ã™ã€‚",
            "â€»ä¿¡ã˜ã‚‹ã‹ä¿¡ã˜ãªã„ã‹ã¯ã€ã‚ãªãŸæ¬¡ç¬¬ã€‚",
            "â€»AIãŒã‚ãªãŸã®é‹å‘½ã‚’è¦‹å®ˆã£ã¦ã„ã¾ã™ã€‚"
        ]))
